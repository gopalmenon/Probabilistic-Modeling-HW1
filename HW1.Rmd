---
title: 'CS6190: Probabilistic Modeling Homework 1
Exponential Families, Conjugate Priors'
author: "Gopal Menon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{mathtools}
- \usepackage{amsmath}
- \usepackage{dirtytalk}
- \DeclareMathOperator{\Unif}{Unif}
- \DeclareMathOperator{\E}{E}
- \DeclareMathOperator{\Var}{Var}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\hrule

\section*{Written Part}

\begin{enumerate}
\item \textbf{Expectation of sufficient statistics:} Consider a random variable $X$ from a continuous exponential family with natural parameter $\eta=\left ( \eta_1,\ldots, \eta_n \right)$. Recall this means the pdf is of the form:

$$
p(x) = h(x) \exp\left (\eta \cdot T(x) -A(\eta)\right)
$$

\begin{enumerate}

\item Show that $E\left[ T(x)|\eta\right] = \nabla A(\eta) = \left(\frac{\partial A}{\partial \eta_1}, \ldots,\frac{\partial A}{\partial \eta_d}  \right)$.

\textbf{Hint:} Start with the identity $\int p(x) dx = 1$, and take the derivative with respect to $\eta$.

$$ \begin{aligned} 
\int p(x) dx &= 1\\
\Rightarrow \int h(x) \exp\left (\eta \cdot T(x) -A(\eta)\right) dx &= 1\\
\nabla \left( \int h(x) \exp\left (\eta \cdot T(x) -A(\eta)\right) dx\right) &= 0\\
\int h(x) \exp\left (\eta \cdot T(x) -A(\eta)\right) . \left(T(x) - \nabla A(\eta) \right) dx &= 0\\
\int T(x) h(x) \exp\left (\eta \cdot T(x) -A(\eta)\right)dx &= \nabla A(\eta)\int h(x) \exp\left (\eta \cdot T(x) -A(\eta)\right)dx\\
\int T(x) p(x) dx &= \nabla A(\eta) \int p(x) dx\\
E\left[ T(x)\right] &= \nabla A(\eta)
\end{aligned}
$$

\item Verify this formula works for the Gaussian distribution with unknown mean, $\mu$, and known variance, $\sigma^2$.\\

\textbf{Hint:} Start by thinking about what the natural parameter $\eta$ and the function $A(\eta)$ are, then verify that the expectation of the Gaussian is the same as $\nabla A(\eta)$.

The pdf for a Gaussian distribution with unknown mean, $\mu$, and known variance, $\sigma^2$ is given by:

$$
\begin{aligned} 
p(x) &= \frac{1}{\sqrt{2\pi}\sigma} \exp \left [ -\frac{(x-\mu)^2}{2\sigma^2} \right ]\\
&= \frac{1}{\sqrt{2\pi}} \exp \left [ \ln \left(\frac{1}{\sigma}\right) \right ] \exp \left [ -\frac{(x-\mu)^2}{2\sigma^2} \right ]\\
&= \frac{1}{\sqrt{2\pi}} \exp \left [ -\ln \left(\sigma\right)  -\frac{(x-\mu)^2}{2\sigma^2} \right ]\\
&= \frac{1}{\sqrt{2\pi}} \exp \left [ -\ln \left(\sigma\right) - \frac{x^2}{2\sigma^2} + \frac{2x\mu}{2\sigma^2} -\frac{\mu^2}{2\sigma^2} \right ]\\
&= \frac{1}{\sqrt{2\pi}} \exp \left [ \frac{x\mu}{\sigma^2} - \frac{x^2}{2\sigma^2} - \left (\ln \left(\sigma\right) + \frac{\mu^2}{2\sigma^2} \right) \right]\\
&= \frac{1}{\sqrt{2\pi}} \exp \left [\begin{bmatrix} \frac{\mu}{\sigma^2} \\ \frac{-1}{2\sigma^2} \end{bmatrix} \cdot  \begin{bmatrix} x \\ x^2 \end{bmatrix} - \left (\ln \left(\sigma\right) + \frac{\mu^2}{2\sigma^2} \right) \right ]
\end{aligned}\\
$$

Based on the form of the exponential family pdf, we can see that:
$$
\begin{aligned} 
A(\eta) &= \ln \left(\sigma\right) + \frac{\mu^2}{2\sigma^2}, \\
\eta &= \begin{bmatrix} \frac{\mu}{\sigma^2} \\ \frac{-1}{2\sigma^2} \end{bmatrix}, \text{ and}\\
T(x) &= \begin{bmatrix} x \\ x^2 \end{bmatrix}\\
\end{aligned}\\
$$

Based on the equation for $E\left[T(x)\right]$ above:
$$
\begin{aligned} 
E\left[ T(x)\right] &= \nabla A(\eta)\\
\Rightarrow E \left[ \begin{bmatrix} x \\ x^2 \end{bmatrix}\right] &= \begin{bmatrix}\frac{\partial A}{\partial \eta_1} \\\frac{\partial A}{\partial \eta_2}  \end{bmatrix}, \text{ where }\eta_1 =  \frac{\mu}{\sigma^2}, \text{ and } \eta_2 = \frac{-1}{2\sigma^2}\\
\begin{bmatrix} E\left[x\right] \\ E\left[x^2\right] \end{bmatrix} &= \begin{bmatrix}\frac{\partial \left( \ln \left(\sigma\right) + \frac{\mu^2}{2\sigma^2}  \right)}{\partial \eta_1} \\\frac{\partial \left( \ln \left(\sigma\right) + \frac{\mu^2}{2\sigma^2}  \right)}{\partial \eta_2}  \end{bmatrix}\\
&= \begin{bmatrix}\frac{\partial \left( \ln \left(\sigma\right) + \frac{\sigma^2}{2}\frac{\mu^2}{\sigma^4}  \right)}{\partial \eta_1} \\\frac{\partial \left( \ln \left(\sqrt{\frac{-1}{2\eta_2}}  \right) - \mu^2\eta_2  \right)}{\partial \eta_2}  \end{bmatrix}\\
&= \begin{bmatrix}\frac{\partial \left( \ln \left(\sigma\right) + \frac{\sigma^2}{2}\eta_1^2  \right)}{\partial \eta_1} \\\frac{\partial \left( \ln \left(\frac{i}{\sqrt{2}\sqrt{\eta_2}}  \right) - \mu^2 \eta_2  \right)}{\partial \eta_2}  \end{bmatrix}\\
&= \begin{bmatrix}\frac{\sigma^2}{2}.2\eta_1 \\ \frac{\sqrt{2}\sqrt{\eta_2}}{i}.\frac{i}{\sqrt{2}}.\frac{-1}{2} \eta_2^{\frac{-3}{2}} -\mu^2  \end{bmatrix}\\
&= \begin{bmatrix}\frac{\sigma^2}{2}.2\frac{\mu}{\sigma^2} \\ \frac{-1}{2\eta_2} -\mu^2  \end{bmatrix}\\
&= \begin{bmatrix}\mu \\ \sigma^2 - \mu^2  \end{bmatrix}\\
\Rightarrow E\left[x\right] &= \mu, \text{ and}\\
 E\left[x^2 \right] &= \sigma^2 - \mu^2, \text{ which is true since } \sigma^2 = E\left[x^2 \right] - \mu^2
\end{aligned}
$$

\end{enumerate}

\item \textbf{Noninformative priors for the Poisson distribution:} Let $X \sim Pois(\lambda)$. Recall that the pmf of the Poisson is

$$
P(X=k;\lambda)=\frac{\lambda^k e^{-\lambda}}{k!}
$$

\begin{enumerate}

\item Rewrite the above pmf in exponential family form. What is the natural parameter? What is the sufficient statistic?

$$
\begin{aligned} 
P(X=k;\lambda)&=\frac{\lambda^k e^{-\lambda}}{k!}\\
&=\frac{1}{k!}\exp\left(\ln{\lambda^k e^{-\lambda}}\right)\\
&= \frac{1}{k!}\exp\left(k\ln\lambda -\lambda\right)\\
\eta &= \ln \lambda, \text{ natural parameter}\\
h(k) &= k, \text{ sufficient statistic}
\end{aligned}
$$

\item Give at least two different options for noninformative priors for $p(\lambda)$.

\begin{enumerate}

\item \textbf{Uniform Prior:} In this case the prior is

$$
\begin{aligned} 
p(\lambda) &= c, \text{ for } 0\leq x \leq \infty, \text{, where c is a constant}
\end{aligned}
$$



\item \textbf{Jeffrey's Prior:} In this case the prior is

$$
\begin{aligned} 
\mathcal{I}(\lambda) &= E\left[ \left(\frac{\partial}{\partial\lambda} \ln  p(x|\lambda)  \right )^2 \right]\\
&= -E\left[ \frac{\partial^2}{\partial\lambda^2} \ln  p(x|\lambda)  \right]\\
&= -E\left[ \frac{\partial^2}{\partial\lambda^2} \ln  \frac{\lambda^xe^{-\lambda}}{x!}   \right]\\
&= -E\left[ \frac{\partial^2}{\partial\lambda^2} \left ( x \ln \lambda - \lambda - \ln x! \right)   \right]\\
&= E\left[ \frac{\partial}{\partial\lambda}\left( \frac{x}{\lambda} - 1   \right ) \right]\\
&=-E\left[\frac{-x}{\lambda^2}\right]\\
&=E\left[\frac{x}{\lambda^2}\right]\\
&=\frac{1}{\lambda^2}E\left[x\right]\\
&= \frac{1}{\lambda^2} \sum_{i=1}^{\infty} x p(x)\\
&= \frac{1}{\lambda^2} \sum_{i=1}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!}\\
&= \frac{e^{-\lambda}}{\lambda} \sum_{i=1}^{\infty}\frac{\lambda^{x-1} }{(x-1)!}\\
&= \frac{e^{-\lambda}}{\lambda} \left( \frac{\lambda^0}{0!} + \frac{\lambda^1}{1!} + \frac{\lambda^2}{2!}+\ldots\right)\\
&= \frac{e^{-\lambda}}{\lambda} e^{\lambda}\\
&= \frac{1}{\lambda}\\
p(\lambda) &= \sqrt{\mathcal{I}(\lambda)}\\
&= \sqrt{\frac{1}{\lambda}}
\end{aligned}
$$
\end{enumerate}

\item What are the resulting posteriors for your two options? Are they proper (i.e., can they
be normalized)?
\begin{enumerate}

\item \textbf{Uniform Prior:} 
The posterior is
$$
\begin{aligned} 
p(\lambda|x) &\propto p(x|\lambda)p(\lambda)\\
&= \prod_{i=1}^n \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}.c, \text{based on the assumtion of i.i.d. samples}\\
&\propto\prod_{i=1}^n \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}
\end{aligned}
$$
Even though the prior cannot be normalized, since it will not add up to $1$, the posterior shown above, where the constant $c$ has been dropped, can be normalized. The reason is that the posterior is the product of distributions that can themselves be normalized.

\item \textbf{Jeffrey's Prior:} The posterior is
$$
\begin{aligned} 
p(\lambda|x) &\propto p(x|\lambda)p(\lambda)\\
&= \prod_{i=1}^n \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}.\sqrt{\frac{1}{\lambda}}, \text{based on the assumtion of i.i.d. samples}\\
&=\prod_{i=1}^n \frac{\lambda^{x_i-\frac{1}{2}}e^{-\lambda}}{x_i!}
\end{aligned}
$$
This can be normalized since it is the product of the likelihood, which is a product of Poisson distributions, and so can be normalized, and the prior, which can also be normalized. 
\end{enumerate}

\end{enumerate}

\item \textbf{Non-conjugate priors:} Let $X_i$ be from a Gaussian with known variance $\sigma^2$ and mean $\mu$
with uniform prior, i.e.,

$$
\begin{aligned} 
\mu &\sim Unif(a,b)\\
X_i &\sim N(\mu,\sigma^2)
\end{aligned}
$$
What is the posterior pdf, $p(\mu|x_1,\ldots,x_n;\sigma^2,a,b)$?

\textbf{Hint:} There will be an integral that you wonâ€™t be able to analytically solve (just leave it in integral form).

$$
\begin{aligned} 
p(\mu|x_1,\ldots,x_n;\sigma^2,a,b) &= p(x_1,\ldots,x_n|\mu;\sigma^2)p(\mu;a,b)\\
&=\int_{-\infty}^{\infty}p(x|\mu;\sigma^2)p(\mu;a,b)dx\\
&= \int_a^bp(x|\mu;\sigma^2)\frac{1}{b-a}dx\\
&= \frac{1}{b-a}\frac{1}{\sqrt{2\pi}\sigma}\int_a^b \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right )dx 
\end{aligned}
$$
\end{enumerate}